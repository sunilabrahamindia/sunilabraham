---
layout: default
title: "#NAMAprivacy: Regulating Artificial Intelligence Algorithms"
description: "A MediaNama report by Aroon Deep documenting discussions on artificial intelligence regulation, algorithmic bias, and fairness from MediaNama's privacy conference in Bangalore, featuring experts from CIS, IFMR, Vidhi Centre, and industry practitioners."
categories: [Artificial Intelligence, Media mentions]
date: 2017-10-18
authors: ["Aroon Deep"]
source: "MediaNama"
permalink: /media/namaprivacy-regulating-ai-algorithms-medianama/
created: 2026-01-07
---

**#NAMAprivacy: Regulating Artificial Intelligence Algorithms** is a *MediaNama* article published on 18 October 2017 by Aroon Deep. The report documents the second part of discussions from MediaNama's privacy conference held in Bangalore on 5 October, examining regulatory approaches to artificial intelligence and algorithmic decision-making. The article features contributions from Sunil Abraham and Pranesh Prakash of the Centre for Internet & Society, alongside researchers, legal experts, and industry practitioners debating fairness, transparency, and the feasibility of regulating opaque algorithmic systems.

## Contents

1. [Article Details](#article-details)
2. [Full Text](#full-text)
3. [Context and Background](#context-and-background)
4. [External Link](#external-link)

## Article Details

<dl class="media-details">
<dt>ðŸ“° Published in:</dt>
<dd><em>MediaNama</em></dd>

<dt>ðŸ“… Date:</dt>
<dd>18 October 2017</dd>

<dt>ðŸ‘¤ Authors:</dt>
<dd>Aroon Deep</dd>

<dt>ðŸ“„ Type:</dt>
<dd>Conference Report</dd>

<dt>ðŸ“° Article Link:</dt>
<dd>
<a class="btn" href="https://www.medianama.com/2017/10/223-namaprivacy-regulating-artificial-intelligence-algorithms/">
Read Online
</a>
</dd>
</dl>

## Full Text

<div class="highlighted-text" id="fulltext">

<p><em>On 5th October, MediaNama held a #NAMAprivacy conference in Bangalore focused on Privacy in the context of Artificial Intelligence, Internet of Things and the issue of consent, supported by Google, Amazon, Mozilla, ISOC, E2E Networks and Info Edge, with community partners HasGeek and Takshashila Institution. Part 1 of the notes from the discussion on AI and Privacy are <a href="https://www.medianama.com/2017/10/223-namaprivacy-artificial-intelligence-privacy/" rel="nofollow noopener noreferrer">here</a>. Part 2:</em></p>

<p>In a matter of decades, algorithms have gone from solving simple math equations to processing immense volumes of data to throw up analytics that even the creators of the algorithms have trouble dealing with. So what do you do when these algorithms create and process some of our most sensitive data? How do we regulate them, if we can do so at all?</p>

<h2>How algorithms deepen bias</h2>

<p><strong>Beni Chugh, a Research Associate at the IFMR Research Foundation</strong>, explained how algorithms can amplify prejudices that people already have: "Our data is our window to our informational privacy. Around ourselves, we see that a lot of aggregation, deep machine learning, algorithms, AI, and such semantics are now the templates of how businesses work. So how does it impact the customer, and is it really a point of tension? The point of tension is only this: that <strong>it amplifies certain biases that humans already had</strong>; however, data networks, the way algorithms are designed in stages, they tend to be very highly path-dependent. Therefore <strong>some of these biases tend to get amplified over stages</strong>, and they become structured biases that can lead to discrimination, which may not be always nefarious or intentional, but it does happen, and even more, unfortunately, it is very hard to detect.</p>

<p>"So that is one kind of harm and challenge, that while in the absence of traditional financial data, our financial access is the poster-child of the marvels of technology, really. So we've been able to generate credit for those who were left out. However <strong>now that we have so much more data about them, are we at the risk of excluding the excluded once more?</strong>"</p>

<h2>Regulating outcomes</h2>

<p>Every regulation has to come from a sense of what is fair and what is unfair â€” how is that possible with algorithms which often run opaquely with unclear outcomes? Chugh suggested that the constitution and the law provided a decent foundation to understand what is fair or unfair, and provided the example of how usury â€” lending at really high-interest rates â€” was outlawed because the consensus was that it was unfair.</p>

<p>But stepping back, should fairness be evaluated while an algorithm is fed data, or while it is processing that data, or while it is spitting out results?</p>

<p><strong>Pranesh Prakash, Policy Director at the Centre for Internet & Society (CIS)</strong>, argued: "I wouldn't like to define fairness itself but rather than doing that I would like to point out that perhaps <strong>we shouldn't really be looking for fairness in the algorithm, but rather in outcomes</strong>. So the outputs of algorithms are used. Algorithms in and of themselves don't do things. It's what the algorithms spit out which are then used. Are we using them in a fair manner so it â€” at the end of the day boils down to human beings and how we as a society may look at fairness and not really at algorithms themselves. <strong>With algorithms what we need is awareness. We shouldn't look at algorithms as black boxes where things happen, we don't know what is happening really, and something comes out</strong>. If that is the situation, then we shouldn't ever be using that kind of an algorithm in institutions where we need notions of fairness. For instance, while providing credit, we can't use black box algorithms. While delivering justice in a court setting, we can't use black box algorithms."</p>

<p><strong>Sunil Abraham, also from CIS</strong>, said: "Unlike something much simpler from a regulatory standpoint like child pornography, <strong>AI is a full-spectrum regulatory question</strong>. With child pornography, it is an absolute prohibition for everybody. So it's a very simple regulatory regime. With AI, we need absolute prohibition in some cases and absolute forbearance in other cases. In some cases, we need absolute prohibition in some cases because we can't use algorithms which we don't understand, but in some cases transparency would be sufficient, to have an open source code."</p>

<h2>Regulating it all</h2>

<p>Asked which needed to be regulated â€” input, process, or outcome â€” Chugh replied, <strong>"The answer is a very sad all of the above."</strong></p>

<p>"Every jurisdiction, given their own ethical understandings, arrives at a common public policy stance. Maybe for India, it may make sense not to use caste data for commercial purposes. So maybe the government is the sole custodian of caste data. In that instance, there may be merit in limiting something right at the collection stage, because once it is collected it is very difficult to ensure that it's never used. So there may be genetic information, maybe. Could be another example which is never collected.</p>

<p>"Then you come to the processing stage, and you realize certain kinds of intended business reasons themselves are not legitimate enough so these are these new studies that are coming out which can actually predict the sexual orientation of a person by just looking at their photographs. <strong>Do we find any social or business merit compelling enough for us to identify people's such private personality traits by just looking at their photographs?</strong> So that's a processing kind of principle that you will need there that what is it that you're processing it for? And ultimately outcomes, because systems are not perfect. There will be biases, unintentionally, which may be visible only over time, and we may find some merit in <strong>regulating for certain kind of outcomes and say they are not permissible</strong>."</p>

<h2>Who is being regulated</h2>

<p><strong>Vinayak Hegde a consultant for big data and associated with ZoomCar</strong>, pointed out that regulation would be targeting only those institutions which can deal with regulation sufficiently: "Who are your adversaries? It's the question of the individual against state or corporation. Who's the powerful one, and how is the balance of power changing really fast? <strong>The adversaries are well-funded, have no incentive to listen to you</strong>. In such a world, the question for me honestly is â€” and we often have this conversation â€” are we moving to a world that looks like 1984 designed by George Orwell, or is it like the one called Brave New World which is by Aldous Huxley? And the answer is maybe a bit of both. So for me I think the best that we can do is probably we are on a slippery slope, we can make it less slippery. Because the problems are very fundamental, what is fairness is very hard to define. The second thing is <strong>we don't even know how algorithms work</strong>. And increasingly moving to a world where the cost of producing data is very low, the cost of processing it is really low, but historically we have done really badly at agreeing at some certain principles, that is why we have constant wars going around."</p>

<p>Alok Prasanna Kumar from the Vidhi Centre for Legal Policy said: "We shouldn't be focused on 'oh, but private companies are so powerful'. <strong>Actually, the government is very powerful</strong>. Do you want the government to put in place a horribly discriminatory system of denying people benefits just because it uses an algorithm? I don't think so. I think we should really be worried about how the law regulates the individuals and institutions who create and use algorithms, not the algorithms themselves. There's no way the law is going to tell you, this is a good algorithm and this is a bad algorithm."</p>

<h2>Regulation is possible</h2>

<p>Alok Prasanna Kumar also pointed out that similar complex industries have already successfully faced regulation: "Pharmaceutical drugs in the sense that given the complexity involved in discovering a molecule and putting it to use, there were questions like, 'can regulators really do it?' They can. You get it right sometime, you get it wrong sometimes, but it's possible to do it, there's no question of impossibility. Today, we have the US FDA which comes in and inspects factories in India and we're perfectly okay with it â€” so much for jurisdiction and all of that â€” and there are global treaties for it.</p>

<p>"Likewise, as Beni [Chugh] mentioned, who knows, tomorrow we may have a data treaty, that all countries sign up to. I think there are several models for us to choose from. <strong>Technology will always be something that the law looks back on and goes, 'oh no, what do we do about this?'</strong> but in any reasonably functioning society, we'll figure out the answers depending on what we want to be done."</p>

<p>Sunil Abraham from CIS pointed out: "If there is a requirement that you must have a discrimination impact assessment as part of the regulatory regime, it's just like an environmental impact assessment. Before you launch the dam or the hotel, you have to demonstrate yourself that you have checked whether your algorithms have any kind of disparate impact on protected classes.</p>

<p><strong>For environment, we have managed, which is much more complex</strong>. If you were to think about it from the perspective of datasets and types of data and types of impact, this is just asking, 'does the algorithm have a different impact on upper and lower caste people?', much simpler to prove as a techie than to prove that the dam will do this to the forest, or the dam will do something else. It's a much more simpler question to answer, according to me."</p>

<h2>Formats of regulating algorithms</h2>

<p>Beni Chugh from IFMR Finance Foundation gave an idea of what regulating algorithms might look like: "Principle-level understanding is essential but I think that a regulatory toolkit will need to first have principle-level abstraction bounds, the first step to which was the Supreme Court privacy judgement, and there is a lot of thinking around what should be permissible, what should not be permissible. But <strong>the ideal toolkit will have technology fixes, data security fixes</strong>. There is no one single answer, so <strong>we could come in and say that the principle-level understanding is privacy by design and baked-in privacy</strong>, and that's where the data security people become absolutely indispensable, and that's when they ensure that we are able to deal with that in varying contexts."</p>

<p>Chugh acknowledged that most of this would have to be self-regulation that data controllers themselves take upon themselves, and in "designing algorithms for de-biasing algorithms".</p>

<p>Akash Mahajan from Appsecco had a skeptical view of regulation: "Policy is awesome. Enforcement is not. Process is brilliant. There is hardly anyone who'd disagree that process doesn't work. But only when it works. The real world is just bonkers."</p>

</div>

<button class="copy-btn-full" data-copytarget="#fulltext">Copy Full Text</button>

{% include back-to-top.html %}

## Context and Background

This discussion occurred shortly after the Supreme Court of India's landmark judgement in *Justice K.S. Puttaswamy (Retd.) v. Union of India* on 24 August 2017, which recognised privacy as a fundamental right under the Constitution. The judgement had significant implications for how personal data and algorithmic decision-making systems would be evaluated, establishing constitutional foundations for subsequent debates about data protection and algorithmic accountability.

In 2017, machine learning systems were increasingly deployed across financial services, governance, and commercial applications, yet regulatory frameworks remained largely absent. Concerns about algorithmic bias had begun attracting scholarly attention internationally, particularly following investigations into criminal justice risk assessment tools and credit scoring systems that demonstrated disparate impacts on protected groups. However, India lacked specific legislative provisions addressing algorithmic transparency or discrimination.

The conference participants grappled with fundamental questions about whether algorithms themselves warranted regulation or whether focus should rest on their creators, operators, and consequences. Chugh's reference to regulating data collection, processing, and outcomes reflected recognition that algorithmic harms could manifest at multiple stages. Abraham's comparison to environmental impact assessments suggested adapting existing regulatory paradigms rather than constructing entirely novel frameworks.

The debate between Hegde and Kumar about regulatory targetsâ€”private corporations versus state actorsâ€”highlighted anxieties about concentrated power in both spheres. India's Aadhaar programme, which employed biometric authentication systems for welfare delivery, exemplified governmental algorithmic deployment raising fairness concerns. Simultaneously, commercial platforms were accumulating vast datasets with limited oversight.

Kumar's pharmaceutical regulation analogy acknowledged regulatory complexity whilst asserting feasibility. His reference to the US FDA inspecting Indian manufacturing facilities illustrated how jurisdictional boundaries had already been navigated in other domains requiring technical expertise and global coordination. This discussion predated India's eventual passage of the Digital Personal Data Protection Act by six years, occurring during a formative period when frameworks for algorithmic governance remained largely conceptual.

## External Link

- <a href="https://www.medianama.com/2017/10/223-namaprivacy-regulating-artificial-intelligence-algorithms/">Read on MediaNama</a>

<style>
.media-details {
background: #f9fbfe;
border: 1px solid #d8e2f0;
border-radius: 10px;
padding: 1.2rem 1.4rem;
max-width: 700px;
margin: 1.2rem auto;
font-size: 0.96rem;
line-height: 1.5;
color: #333;
box-shadow: 0 2px 4px rgba(0,0,0,0.04);
}
.media-details dt {
font-weight: 600;
color: #1b2a49;
margin-top: 0.7rem;
}
.media-details dd {
margin: 0 0 0.3rem 0.3rem;
color: #555;
}
.highlighted-text {
background-color: #fffbea;
border-left: 4px solid #f2ce61;
padding: 1rem 1.2rem;
border-radius: 8px;
line-height: 1.65;
color: #333;
box-shadow: 0 1px 3px rgba(0,0,0,0.05);
margin-bottom: 0.8rem;
}
.highlighted-text p {
margin-bottom: 1rem;
}
.copy-btn-full {
display: inline-block;
background: #f1f1f1;
border: 1px solid #ccc;
font-size: 0.85rem;
padding: 0.4rem 0.8rem;
border-radius: 6px;
cursor: pointer;
transition: background 0.2s ease;
margin-bottom: 1.5rem;
}
.copy-btn-full:hover {
background: #e5e5e5;
}
</style>

<script>
document.addEventListener('DOMContentLoaded', () => {
document.querySelectorAll('.copy-btn-full').forEach(btn => {
btn.addEventListener('click', async () => {
const target = document.querySelector(btn.getAttribute('data-copytarget'));
if (!target) return;
try {
await navigator.clipboard.writeText(target.innerText.trim());
const original = btn.textContent;
btn.textContent = 'Copied!';
setTimeout(() => (btn.textContent = original), 1500);
} catch (e) {
btn.textContent = 'Copy failed';
setTimeout(() => (btn.textContent = 'Copy Full Text'), 1500);
}
});
});
});
</script>
