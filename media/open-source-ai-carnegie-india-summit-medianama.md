---
layout: default
title: "Navigating the Complexities of Open Source AI: Insights from Carnegie India Summit"
description: "A MediaNama report by Kamya Pandey examining debates around open source artificial intelligence, featuring perspectives from Meta and the Digital Public Goods Alliance at Carnegie India's Global Technology Summit."
categories: [Artificial Intelligence, Media mentions]
date: 2023-12-08
authors: ["Kamya Pandey"]
source: "MediaNama"
permalink: /media/open-source-ai-carnegie-india-summit-medianama/
created: 2026-01-07
---

**Navigating the Complexities of Open Source AI: Insights from Carnegie India Summit** is a *MediaNama* article published on 8 December 2023 by Kamya Pandey. The report examines discussions from Carnegie India's Global Technology Summit held in Delhi between 4-6 December, focusing on definitional challenges surrounding open source artificial intelligence. The article features perspectives from Sunil Abraham, Meta's Public Policy Director for Data Governance and Emerging Technologies, and Lea Gimpel from the Digital Public Goods Alliance, exploring tensions between transparency, safety, and innovation in AI development.

## Contents

1. [Article Details](#article-details)
2. [Full Text](#full-text)
3. [Context and Background](#context-and-background)
4. [External Link](#external-link)

## Article Details

<dl class="media-details">
<dt>ðŸ“° Published in:</dt>
<dd><em>MediaNama</em></dd>

<dt>ðŸ“… Date:</dt>
<dd>8 December 2023</dd>

<dt>ðŸ‘¤ Authors:</dt>
<dd>Kamya Pandey</dd>

<dt>ðŸ“„ Type:</dt>
<dd>News Report</dd>

<dt>ðŸ“° Article Link:</dt>
<dd>
<a class="btn" href="https://www.medianama.com/2023/12/223-open-source-ai-carnegie-india-summit/">
Read Online
</a>
</dd>
</dl>

<figure class="media-image">
  <img
    src="https://raw.githubusercontent.com/sunilabrahamindia/sunilabraham/main/media/images/open-source-ai-carnegie-india-summit-medianama.jpeg"
    alt="Panel discussion at the Carnegie India Global Technology Summit 2023, focused on open source artificial intelligence and technology governance.">
  <figcaption>Panel discussion on open source AI at the Carnegie India Global Technology Summit, December 2023.</figcaption>
</figure>

## Full Text

<div class="highlighted-text" id="fulltext">

<p>"Most economists will agree that open source software is a digital public good because it meets the definition, the economist definition of what is a public good," Public Policy Director at Meta Sunil Abraham said during Carnegie India's Global Technology Summit that took place in Delhi between 4-6 December. Abraham said this while addressing the debate between open-sourced and closed-source artificial intelligence models.</p>

<p>Interestingly, Meta's large language model (LLM, a type of artificial intelligence program) LLaMA 2 which was launched in July this year is open-sourced. Abraham said that even outside of LLaMA 2, Meta's founder Mark Zuckerberg has been committed to using open-source software from the very beginning. "And in that way, corporations like Meta can contribute to India's vision around digital public infrastructure by providing the building blocks for that digital public infrastructure, which is open source software or digital public goods," he explained. He mentioned that the company has 1200 open-source projects.</p>

<p><strong>Wait, what does open source mean?</strong></p>

<p>The term open source refers to something people can modify and share because its design is publicly accessible. The term owes its origins to software development and is used to describe software with source code that anyone can inspect, modify, and enhance. But it doesn't have such a clear definition when it comes to artificial intelligence (AI). "There is no agreed-upon open source definition for AI so that's something we need to acknowledge. I would say, and others say this as well, that LLaMA for instance wouldn't qualify as open source under the open source software definition because they use restrictions baked into the license," Lea Gimpel from the Digital Public Goods Alliance explained.</p>

<p>What Gimpel meant was that different companies have different interpretations of what open-sourcing means for AI. For instance, OpenAI's ChatGPT is open access in that anyone can use the application programming interface (API) to build applications. But neither the datasets used to train the model nor the codebase are public. OpenAI also uses different definitions of what open source means for its different projects and has in some cases. According to Gimpel, the fact that LLaMA 2 has restrictions on how you can use it baked into its license means that it wouldn't qualify as open-source software.</p>

<p><strong>How open source AI can be defined:</strong></p>

<p>"There are currently several work streams and ways that are trying to define open source AI with the community to better understand what would we actually need to open source in order to maintain the benefits that we see in open source software," Gimpel said. She explained that the factors being considered in the definition are transparency, the ability to modify a system, and reproducibility.</p>

<p>Gimpel explained that from the Digital Public Goods Alliance's perspective, the focus on open-source AI discussions should be on transparency and accountability, as opposed to reproducibility. She argued that to enable others to build on top of an existing AI model, you don't need to give them access to fully trained datasets. "What you probably need [to give] is a good understanding of what the data is that the model was trained on," she explained adding that this could mean a snapshot of what is representative of the data used for training. "I think what we really need to solve for is what is responsible openness in that sense. Where can you make components openly accessible and do this in a responsible way? And where should you probably also keep it closed," Gimpel said.</p>

<p><strong>Intersections between open source and transparency:</strong></p>

<p>"If there is a regulated entity, a corporation, an academic organization that makes a very specific safety claim about the model that they are releasing, then the only way that we can conclude with scientific confidence that that claim is correct is if we embrace the whole open science paradigm and not just open source," Abraham argued. He said that for independent researchers to ensure the safety of an AI model, its source code and training data sets would need to be publicly available. He also mentioned that the model would have to be released along with a scholarly paper. "And only then can you have full reproducibility in the tradition of science and independently people can confirm a particular safety claim," Abraham explained.</p>

<p>Speaking about Meta's perspective on open sourcing, Abraham said that the company is "non-dogmatic about open source. That means we do have many proprietary products as well. " He explained that when Meta develops an AI model it asks the questionâ€” what risks do these models pose and should this model be openly released or should it remain a proprietary model? "A good example of this is VoiceBox. VoiceBox is a model that can be trained on a three-second sample of anybody's voice. And after that training is done, VoiceBox can artificially synthesize that person's speech, replicating accent, etc. So that model, because of the deepfake risks, we decided not to make it open source," he shared. He explained that the company has "limited, not full source code transparency, but transparency about how much compute was used, how much carbon was burnt, what data sets were used to train the model, etc."</p>

<p><strong>Is open source always the best choice for AI companies?</strong></p>

<p>Gimpel said that data and privacy need to be considered when deciding to open-source an AI model. She explained that developers need to make sure that their AI models do not leak personally identifiable information (PII) before open-sourcing them. "Sadly, we currently don't have any open sourcing standards of when is it to be safe to open source the model. I think that the industry needs to develop that," she said.</p>

<p>The other part that needs to be considered is the misuse of the AI model downstream, that is, by those building on top of an AI model. "In the open source software world we have seen that that's not achievable, you can't monitor the downstream use of an open source project," she said. She said that when considering the same for AI, it's important to identify bottlenecks that everyone working with AI goes through and use those as a means of safeguarding against misuse.</p>

<p>For instance, anyone developing AI needs to go through cloud infrastructure. "One idea could be, for instance, to put the liability on cloud providers to make sure that the model is not misused and they will develop measures accordingly to account for them," she suggested. She also suggested that industries could be asked to obtain a license to operate a specific model. "I guess I would get a lot of bashing from the open source community for saying this because that's clearly not falling under the open source definition anymore at least not in the ideological sense. But it would help to account for these risks and at the same time ensuring that others can build on a model and innovate on top of it," she shared.</p>

</div>

<button class="copy-btn-full" data-copytarget="#fulltext">Copy Full Text</button>

{% include back-to-top.html %}

## Context and Background

This discussion occurred during a transitional period for artificial intelligence governance, when industry actors, civil society organisations, and policymakers grappled with applying traditional open source principles to machine learning systems. Unlike conventional software where source code transparency suffices for the open source designation, AI models involve multiple componentsâ€”training data, model weights, inference code, and computational resourcesâ€”creating definitional ambiguities.

Meta's release of LLaMA 2 in July 2023 exemplified these tensions. Whilst marketed as open source, the model imposed usage restrictions through licensing terms, prompting debate about whether such constraints violated core open source tenets. This divergence from established open source software definitions highlighted the need for AI-specific frameworks that account for safety considerations absent in traditional software contexts.

The Digital Public Goods Alliance's emphasis on "responsible openness" reflected growing recognition that unrestricted access to AI models carries distinct risks. Concerns about deepfake generation, personally identifiable information leakage, and malicious downstream applications prompted reconsideration of blanket transparency mandates. Abraham's reference to Meta's VoiceBox modelâ€”withheld from public release due to voice synthesis risksâ€”illustrated industry calculations balancing innovation enablement against potential harms.

Gimpel's proposals for cloud provider liability and model-specific licensing represented attempts to maintain innovation benefits whilst instituting control points for misuse prevention. These suggestions challenged open source orthodoxy, acknowledging that AI's dual-use potential might necessitate governance mechanisms incompatible with unrestricted access paradigms.

India's digital public infrastructure ambitions, referenced in the discussion, provided additional context. Policymakers sought to position the country as both an AI innovation hub and a regulatory standard-setter, creating interest in frameworks that balanced openness with safety considerations appropriate to India's development priorities.

## External Link

- <a href="https://www.medianama.com/2023/12/223-open-source-ai-carnegie-india-summit/">Read on MediaNama</a>

<style>
.media-details {
  background: #f9fbfe;
  border: 1px solid #d8e2f0;
  border-radius: 10px;
  padding: 1.2rem 1.4rem;
  max-width: 700px;
  margin: 1.2rem auto;
  font-size: 0.96rem;
  line-height: 1.5;
  color: #333;
  box-shadow: 0 2px 4px rgba(0,0,0,0.04);
}
.media-details dt {
  font-weight: 600;
  color: #1b2a49;
  margin-top: 0.7rem;
}
.media-details dd {
  margin: 0 0 0.3rem 0.3rem;
  color: #555;
}
.media-image {
  text-align: center;
  margin: 1.5rem auto;
  max-width: 720px;
}
.media-image img {
  width: 100%;
  height: auto;
  border-radius: 8px;
  box-shadow: 0 2px 6px rgba(0,0,0,0.1);
}
.media-image figcaption {
  font-size: 0.9rem;
  color: #555;
  margin-top: 0.5rem;
}
.highlighted-text {
  background-color: #fffbea;
  border-left: 4px solid #f2ce61;
  padding: 1rem 1.2rem;
  border-radius: 8px;
  line-height: 1.65;
  color: #333;
  box-shadow: 0 1px 3px rgba(0,0,0,0.05);
  margin-bottom: 0.8rem;
}
.highlighted-text p {
  margin-bottom: 1rem;
}
.copy-btn-full {
  display: inline-block;
  background: #f1f1f1;
  border: 1px solid #ccc;
  font-size: 0.85rem;
  padding: 0.4rem 0.8rem;
  border-radius: 6px;
  cursor: pointer;
  transition: background 0.2s ease;
  margin-bottom: 1.5rem;
}
.copy-btn-full:hover {
  background: #e5e5e5;
}
</style>

<script>
document.addEventListener('DOMContentLoaded', () => {
document.querySelectorAll('.copy-btn-full').forEach(btn => {
btn.addEventListener('click', async () => {
const target = document.querySelector(btn.getAttribute('data-copytarget'));
if (!target) return;
try {
await navigator.clipboard.writeText(target.innerText.trim());
const original = btn.textContent;
btn.textContent = 'Copied!';
setTimeout(() => (btn.textContent = original), 1500);
} catch (e) {
btn.textContent = 'Copy failed';
setTimeout(() => (btn.textContent = 'Copy Full Text'), 1500);
}
});
});
});
</script>
